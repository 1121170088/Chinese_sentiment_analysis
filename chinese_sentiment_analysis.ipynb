{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Chinese Sentiment analysis with hotel review data\n",
    "## Dependencies\n",
    "\n",
    "Python 3.5, numpy, pickle, keras, tensorflow, [jieba](https://github.com/fxsjy/jieba)\n",
    "\n",
    "## Optional for plotting\n",
    "\n",
    "pylab, scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to pickle and load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lists of files, positive and negative files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBaseDirPos = \"./data/ChnSentiCorp_htl_ba_6000/pos/\"\n",
    "dataBaseDirNeg = \"./data/ChnSentiCorp_htl_ba_6000/neg/\"\n",
    "positiveFiles = [dataBaseDirPos + f for f in listdir(dataBaseDirPos) if isfile(join(dataBaseDirPos, f))]\n",
    "negativeFiles = [dataBaseDirNeg + f for f in listdir(dataBaseDirNeg) if isfile(join(dataBaseDirNeg, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show length of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2916\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveFiles))\n",
    "print(len(negativeFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a look at what's in a file(one hotel review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[0]\n",
    "with codecs.open(filename, \"r\") as doc_file:\n",
    "    text=doc_file.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test removing stop words\n",
    "Demo what it looks like to tokenize the sentence and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Orginal==:\n",
      "\r",
      "一年多没来过了，再来住，感觉不错，旧瓶装了新酒，标准房里面的设施基本上都换新了，到处都见到笑脸，看来这里才真正是买方市场，我喜欢员工“巴结”客人的感觉，给他个满分。\n",
      "==Tokenized==\tToken count:56\n",
      "\r",
      "一年 多 没来 过 了 ， 再来 住 ， 感觉 不错 ， 旧 瓶装 了 新 酒 ， 标准 房 里面 的 设施 基本上 都 换 新 了 ， 到处 都 见到 笑脸 ， 看来 这里 才 真正 是 买方市场 ， 我 喜欢 员工 “ 巴结 ” 客人 的 感觉 ， 给 他 个 满分 。\n",
      "==Stop Words Removed==\tToken count:24\n",
      "\r",
      "一年 没来 再来 住 感觉 不错 旧 瓶装 新 酒 标准 房 设施 换 新 见到 笑脸 买方市场 喜欢 员工 巴结 客人 感觉 满分\n"
     ]
    }
   ],
   "source": [
    "filename = positiveFiles[110]\n",
    "with codecs.open(filename, \"r\") as doc_file:\n",
    "    text=doc_file.read()\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "print(\"==Orginal==:\\n\\r{}\".format(text))\n",
    "    \n",
    "stopwords = [ line.rstrip() for line in codecs.open('./data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "final =[]\n",
    "seg_list = list(seg_list)\n",
    "for seg in seg_list:\n",
    "    if seg not in stopwords:\n",
    "        final.append(seg)\n",
    "print(\"==Tokenized==\\tToken count:{}\\n\\r{}\".format(len(seg_list),\" \".join(seg_list)))\n",
    "print(\"==Stop Words Removed==\\tToken count:{}\\n\\r{}\".format(len(final),\" \".join(final)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare \"doucments\", a list of tuples\n",
    "Some files contain abnormal encoding characters which encoding GB2312 will complain about. Solution: read as bytes then decode as GB2312 line by line, skip lines with abnormal encodings. We also convert any traditional Chinese characters to simplified Chinese characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for filename in positiveFiles:\n",
    "    text = \"\"\n",
    "    with codecs.open(filename, \"rb\") as doc_file:\n",
    "        for line in doc_file:\n",
    "            try:\n",
    "                line = line.decode(\"GB2312\")\n",
    "            except:\n",
    "                continue\n",
    "            text+=Converter('zh-hans').convert(line)# Convert from traditional to simplified Chinese\n",
    "\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            text = text.replace(\"\\r\", \"\")\n",
    "    documents.append((text, \"pos\"))\n",
    "\n",
    "for filename in negativeFiles:\n",
    "    text = \"\"\n",
    "    with codecs.open(filename, \"rb\") as doc_file:\n",
    "        for line in doc_file:\n",
    "            try:\n",
    "                line = line.decode(\"GB2312\")\n",
    "            except:\n",
    "                continue\n",
    "            text+=Converter('zh-hans').convert(line)# Convert from traditional to simplified Chinese\n",
    "\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            text = text.replace(\"\\r\", \"\")\n",
    "    documents.append((text, \"neg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional step to save/load the documents as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5916\n",
      "('酒店的房间装修得飞常好，空间也很大，可是我非常不能理解的是，300多一晚的酒店居然房间不提供免费上网。另外，枕巾洗得不干净，说实话下次不考虑住了。', 'neg')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment those two lines to save/load the documents for later use since the step above takes a while\n",
    "__pickleStuff(\"./data/chinese_sentiment_corpus.p\", documents)\n",
    "documents = __loadStuff(\"./data/chinese_sentiment_corpus.p\")\n",
    "print(len(documents))\n",
    "print(documents[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input and output for the model\n",
    "Each input (hotel review) will be a list of tokens, output will be one token(\"pos\" or \"neg\"). The stopwords are not removed here since the dataset is relative small and removing the stop words are not saving much traing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize only\n",
    "totalX = []\n",
    "totalY = [str(doc[1]) for doc in documents]\n",
    "for doc in documents:\n",
    "    seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    totalX.append(seg_list)\n",
    "\n",
    "\n",
    "#Switch to below code to experiment with removing stop words\n",
    "# Tokenize and remove stop words\n",
    "# totalX = []\n",
    "# totalY = [str(doc[1]) for doc in documents]\n",
    "# stopwords = [ line.rstrip() for line in codecs.open('./data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "# for doc in documents:\n",
    "#     seg_list = jieba.cut(doc[0], cut_all=False)\n",
    "#     seg_list = list(seg_list)\n",
    "#     Uncomment below code to experiment with removing stop words\n",
    "#     final =[]\n",
    "#     for seg in seg_list:\n",
    "#         if seg not in stopwords:\n",
    "#             final.append(seg)\n",
    "#     totalX.append(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize distribution of sentence length\n",
    "Decide the max input sequence, here we cover up to 60% sentences. The longer input sequence, the more training time will take, but could improve  prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is:  1804\n",
      "60% cover length up to:  68\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Rectangle' object has no property 'normed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-cbd4f41be58e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnormed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m#use this to draw histogram of your data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[0;32m   2856\u001b[0m         \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m         \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2858\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6909\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6910\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6911\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6912\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlbl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6913\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, props)\u001b[0m\n\u001b[0;32m   1060\u001b[0m                     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"set_{k}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m                         raise AttributeError(f\"{type(self).__name__!r} object \"\n\u001b[0m\u001b[0;32m   1063\u001b[0m                                              f\"has no property {k!r}\")\n\u001b[0;32m   1064\u001b[0m                     \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Rectangle' object has no property 'normed'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "h = sorted([len(sentence) for sentence in totalX])\n",
    "maxLength = h[int(len(h) * 0.60)]\n",
    "print(\"Max length is: \",h[len(h)-1])\n",
    "print(\"60% cover length up to: \",maxLength)\n",
    "h = h[:5000]\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words to number tokens, padding\n",
    "Pad input sequence to max input length if it is shorter\n",
    "\n",
    "\n",
    "Save the input tokenizer, since we need to use the same tokenizer for our new predition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 22123\n"
     ]
    }
   ],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in totalX]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input vocab_size:\",vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))\n",
    "__pickleStuff(\"./data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output, array of 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vocab_size: 3\n"
     ]
    }
   ],
   "source": [
    "target_tokenizer = Tokenizer(3)\n",
    "target_tokenizer.fit_on_texts(totalY)\n",
    "print(\"output vocab_size:\",len(target_tokenizer.word_index) + 1)\n",
    "totalY = np.array(target_tokenizer.texts_to_sequences(totalY)) -1\n",
    "totalY = totalY.reshape(totalY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn output 0s and 1s to categories(one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = to_categorical(totalY, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save meta data for later predition\n",
    "maxLength: the input sequence length\n",
    "\n",
    "vocab_size: Input vocab size\n",
    "\n",
    "output_dimen: which is 2 in this example (pos or neg)\n",
    "\n",
    "sentiment_tag: either [\"neg\",\"pos\"] or [\"pos\",\"neg\"] matching the target tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reverse_word_index = {v: k for k, v in list(target_tokenizer.word_index.items())}\n",
    "sentiment_tag = [target_reverse_word_index[1],target_reverse_word_index[2]] \n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, train and save it\n",
    "The training data is logged to Tensorboard, we can look at it by cd into directory \n",
    "\n",
    "\"./Graph/sentiment_chinese\" and run\n",
    "\n",
    "\n",
    "\"python -m tensorflow.tensorboard --logdir=.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 68, 256)           5663488   \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 68, 256)           393984    \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 256)               393984    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,451,970\n",
      "Trainable params: 6,451,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 88s 508ms/step - loss: 0.6163 - accuracy: 0.6403 - val_loss: 0.4586 - val_accuracy: 0.8074\n",
      "Epoch 2/20\n",
      "167/167 [==============================] - 82s 494ms/step - loss: 0.4287 - accuracy: 0.8129 - val_loss: 0.3343 - val_accuracy: 0.8615\n",
      "Epoch 3/20\n",
      "167/167 [==============================] - 83s 495ms/step - loss: 0.3035 - accuracy: 0.8768 - val_loss: 0.3164 - val_accuracy: 0.8699\n",
      "Epoch 4/20\n",
      "167/167 [==============================] - 83s 497ms/step - loss: 0.2573 - accuracy: 0.9001 - val_loss: 0.3041 - val_accuracy: 0.8767\n",
      "Epoch 5/20\n",
      "167/167 [==============================] - 83s 500ms/step - loss: 0.2049 - accuracy: 0.9228 - val_loss: 0.3087 - val_accuracy: 0.8818\n",
      "Epoch 6/20\n",
      "167/167 [==============================] - 83s 498ms/step - loss: 0.1708 - accuracy: 0.9378 - val_loss: 0.3691 - val_accuracy: 0.8868\n",
      "Epoch 7/20\n",
      "167/167 [==============================] - 83s 498ms/step - loss: 0.1469 - accuracy: 0.9440 - val_loss: 0.2935 - val_accuracy: 0.8868\n",
      "Epoch 8/20\n",
      "167/167 [==============================] - 83s 495ms/step - loss: 0.1286 - accuracy: 0.9525 - val_loss: 0.2934 - val_accuracy: 0.8733\n",
      "Epoch 9/20\n",
      "167/167 [==============================] - 83s 495ms/step - loss: 0.1096 - accuracy: 0.9576 - val_loss: 0.4842 - val_accuracy: 0.8919\n",
      "Epoch 10/20\n",
      "167/167 [==============================] - 83s 496ms/step - loss: 0.1162 - accuracy: 0.9551 - val_loss: 0.3436 - val_accuracy: 0.8801\n",
      "Epoch 11/20\n",
      "167/167 [==============================] - 83s 498ms/step - loss: 0.0866 - accuracy: 0.9681 - val_loss: 0.4532 - val_accuracy: 0.8970\n",
      "Epoch 12/20\n",
      "167/167 [==============================] - 83s 499ms/step - loss: 0.0781 - accuracy: 0.9688 - val_loss: 0.3558 - val_accuracy: 0.9037\n",
      "Epoch 13/20\n",
      "167/167 [==============================] - 83s 500ms/step - loss: 0.0716 - accuracy: 0.9728 - val_loss: 0.4584 - val_accuracy: 0.8834\n",
      "Epoch 14/20\n",
      "167/167 [==============================] - 83s 499ms/step - loss: 0.0698 - accuracy: 0.9720 - val_loss: 0.5245 - val_accuracy: 0.8936\n",
      "Epoch 15/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0731 - accuracy: 0.9713 - val_loss: 0.3780 - val_accuracy: 0.8953\n",
      "Epoch 16/20\n",
      "167/167 [==============================] - 84s 502ms/step - loss: 0.0716 - accuracy: 0.9731 - val_loss: 0.3636 - val_accuracy: 0.8834\n",
      "Epoch 17/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0700 - accuracy: 0.9711 - val_loss: 0.4263 - val_accuracy: 0.8868\n",
      "Epoch 18/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0516 - accuracy: 0.9788 - val_loss: 0.5747 - val_accuracy: 0.8885\n",
      "Epoch 19/20\n",
      "167/167 [==============================] - 84s 503ms/step - loss: 0.0561 - accuracy: 0.9763 - val_loss: 0.4509 - val_accuracy: 0.8868\n",
      "Epoch 20/20\n",
      "167/167 [==============================] - 84s 501ms/step - loss: 0.0509 - accuracy: 0.9778 - val_loss: 0.4675 - val_accuracy: 0.8936\n",
      "INFO:tensorflow:Assets written to: ./data/sentiment_chinese_model.HDF5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/sentiment_chinese_model.HDF5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRU object at 0x00000215038DB1D0> has the same name 'GRU' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRU'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRU object at 0x00000215038DBFD0> has the same name 'GRU' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRU'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000215038DB630> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000215011F8128> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=20, verbose=1, callbacks=[tbCallBack])\n",
    "model.save('./data/sentiment_chinese_model.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are prediction code\n",
    "Function to load the meta data and the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "sentiment_tag = None\n",
    "maxLength = None\n",
    "def loadModel():\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./data/sentiment_chinese_model.HDF5/variables/variables')\n",
    "        #model.load_weights('/Users/tanmingxin/Desktop/Chinese_sentiment_analysis-master/data/sentiment_chinese_model.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert sentence to model input, and predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the load model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 68, 256)           5663488   \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 68, 256)           393984    \n",
      "                                                                 \n",
      " gru_15 (GRU)                (None, 256)               393984    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,451,970\n",
      "Trainable params: 6,451,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model weights loaded!\n"
     ]
    }
   ],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some new comments, feel free to try your own\n",
    "The result tuple consists the predicted result and likehood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.88785654)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"不会再来了。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.8622959)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"床上有污渍，房间太挤不透气，空调不怎么好用。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 0.9061275)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间有点小但是设备还齐全，没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.9240764)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"房间还算干净，一般般吧，短住还凑合。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.6010793)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"开始不太满意，前台好说话换了一间，房间很干净没有异味。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.6878349)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"不需要。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 0.9964706)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"服务态度极其差，前台接待好象没有受过培训，连基本的礼貌都不懂，竟然同时接待几个客人；\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
